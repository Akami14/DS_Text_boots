{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "C4kiCAeMmmK-"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Взять ноутбук colab_text_classification_part1.ipynb который разбирали на занятии и добавить пункты которые мы пропустили\n",
        "# 1. Проверьте повысилось ли качество на стандартных подходах при лемматизации/и без неё\n",
        "# 2. Удалите/(замените на тег) из текстов сущности(имена, локации, что-то ещё). Запустите классификатор и модельки на сеточках\n",
        "# 3. Сделайте выводы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OlqOAQmQGXOL",
        "outputId": "dc481899-479f-46e4-beeb-27351b4800ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  imdb.zip\n",
            "  inflating: test.tsv                \n",
            "  inflating: train.tsv               \n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.2/216.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for eli5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m55.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "!wget -O imdb.zip -qq --no-check-certificate \"https://drive.google.com/uc?export=download&id=1vrQ5czMHoO3pEnmofFskymXMkq_u1dPc\"\n",
        "!unzip imdb.zip\n",
        "!pip -q install eli5\n",
        "!pip -q install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uF80-qhXt17P"
      },
      "source": [
        "Тут придется авторизировать запрос к гугл драйву, извините."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lRdS0CM2rSzV",
        "outputId": "d8ebbb2b-439a-40d5-fc8a-5424e3d67d8b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:pydrive is deprecated and no longer maintained. We recommend that you migrate your projects to pydrive2, the maintained fork of pydrive\n"
          ]
        }
      ],
      "source": [
        "# Install the PyDrive wrapper & import libraries.\n",
        "# This only needs to be done once per notebook.\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# Authenticate and create the PyDrive client.\n",
        "# This only needs to be done once per notebook.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1d1-5FwxK53ePwygNWeG7jhsOWZbi5HOv'})\n",
        "downloaded.GetContentFile('train_docs.pkl')\n",
        "\n",
        "downloaded = drive.CreateFile({'id': '1MMOY477t965G0C5DtXeREVp0X85UaNq5'})\n",
        "downloaded.GetContentFile('test_docs.pkl')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqbkG4qoS0AB"
      },
      "source": [
        "# Классификация текстов\n",
        "\n",
        "Начнём с самого простого - анализа тональности текста.\n",
        "\n",
        "Будем классифицировать отзывы с IMDB на положительные/отрицательные.\n",
        "\n",
        "Датасет взят с http://ai.stanford.edu/~amaas/data/sentiment/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Js5kqmWMGqN4",
        "outputId": "9be2ad0f-81ee-44c5-92bd-81c7f680fa19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "is_positive\treview\r\n",
            "0\t\"Dreamgirls, despite its fistful of Tony wins in an incredibly weak year on Broadway, has never been what one would call a jewel in the crown of stage musicals. However, that is not to say that in the right cinematic hands it could not be fleshed out and polished into something worthwhile on-screen. Unfortunately, what transfers to the screen is basically a slavishly faithful version of the stage hit with all of its inherent weaknesses intact. First, the score has never been one of the strong points of this production and the film does not change that factor. There are lots of songs (perhaps too many?), but few of them are especially memorable. The closest any come to catchy tunes are the title song and One Night Only - the much acclaimed And I Am Telling You That I Am Not Going is less a great song than it is a dramatic set piece for the character of Effie (Jennifer Hudson). The film is slick and technically well-produced, but the story and characters are surprisingly thin and lacking in any resonance. There is some interest in the opening moments, watching Jamie Foxx's Svengali-like manager manipulate his acts to the top, but that takes a back seat in the latter portion of the film, when the story conveniently tries to cast him as a villain, despite his having been right from a business stand-point for a good majority of the film. Beyonce Knowles is lovely and sings her songs perfectly well, but is stuck with a character who is basically all surface glitz. Anika Noni Rose as the third member of the Dreamgirls trio literally has nothing to do for the entire film. Eddie Murphy acquits himself well as a singer obviously based on James Brown, but the role is not especially meaty and ultimately has little impact. Foxx would seem ideal casting, but he seems oddly withdrawn and bored. The film's biggest selling point is surely former American Idol contestant/Oscar winner Jennifer Hudson in the central role of Effie White, the temperamental singer who gets booted from the group and makes a triumphant closing act return. For me, Effie has always been a big problem in both the show and the movie. The film obviously wants you to feel sorry for her and rather ham-handedly takes her side, but I have never been sure that this character deserves that kind of devotion. From the start, Effie conducts herself for the most part like an obnoxious, egotistical, self-centered diva, who is more interested in what everyone else can do for her rather than having much vested interest in the group of which she is a part. When she is booted from the group for her unprofessionalism and bad attitude, the charges are more than well-founded, but the stage show/film seem to think Effie should be cut unlimited slack simply because she has a great voice. Even though the film tries to soften some of Effie's harder edges to make her more likable, the charges still stand. Her story becomes more manipulative by suggesting she should have our further sympathy because she is an unwed mother struggling to raise her daughter - using the implication that (much like the talent card) motherhood immediately makes any behavior excusable. Indeed the only big effort the film makes to show Effie's mothering is to tell us about it and then include a scene where she barks at her daughter in the unemployment office, insists that the girl has \"\"no father\"\" and then refuse to look for gainful employment to support them since singing is all she knows. In the hands of a skillful actress, the gaps could perhaps have been remedied with technique and charisma. Unfortunately, Hudson is not that actress. She sings well, but the dialog-driven moments do not come naturally to her nor do high emotional moments. Effie's signature moment (the aforementioned And I Am Telling You... number) is well-sung by Hudson, but emotionally flat in the acting department. Effie is supposed to expressing her rage and desperation at her predicament, but Hudson comes off as a cabaret performer belting out a hot number. All in all, not quite the emotional highlight one expects. The latter portion of the film is basically a predictable melange of events that maneuver Foxx into Hudson's earlier position and allow her to strut back in and lord it over everyone. Foxx's criminal offenses in the film are undoubtedly par for the course of many struggling record producers, but the film's seeming implication that he has it coming because he helped usher in the disco era is rather ridiculous, not to mention pretentious and condescending, particularly coming from a film with all of the depth of a puddle. The end result is a faithful rendition of the stage hit, drained of emotion, energy or anything that can be described as dynamic.\"\r\n",
            "0\tThis show comes up with interesting locations as fast as the travel channel. It is billed as reality but in actuality it is pure prime time soap opera. It's tries to use exotic locales as a facade to bring people into a phony contest & then proceeds to hook viewers on the contestants soap opera style.<br /><br />It also borrows from an early CBS game show pioneer- Beat The Clock- by inventing situations for its contestants to try & overcome. Then it rewards the winner money. If they can spice it up with a little interaction between the characters, even better. While the game format is in slow motion versus Beat The Clock- the real accomplishment of this series is to escape reality. <br /><br />This show has elements of several types of successful past programs. Reality television, hardly, but if your hooked on the contestants, locale or contest, this is your cup of tea. If your not, this entire series is as I say, drivel dripping with gravy. It is another show hiding behind the reality label which is the trend it started in 2000.<br /><br />It is slick & well produced, so it might last a while yet. After all, so do re-runs of Gilligan's Island, Green Acres, The Beverly Hillbillies & The Brady Bunch. This just doesn't employ professional actors. The intelligence level is about the same.\r\n",
            "1\tI simply love this movie. I also love the Ramones, so I am sorta biased to begin with in the first place. There isn't a lot of critical praise to give this film, either you like it or you don't. I think it's a great cult movie.\r\n",
            "0\t\"Spoilers ahead if you want to call them that...<br /><br />I would almost recommend this film just so people can truly see a 1/10. Where to begin, we'll start from the top...<br /><br />THE STORY: Don't believe the premise - the movie has nothing to do with abandoned cars, and people finially understanding what the mysterious happenings are. It's a draub, basic, go to cabin movie with no intensity or \"\"effort\"\".<br /><br />THE SCREENPLAY: I usually give credit to indie screenwriters, it's hard work when you are starting out...but this is crap. The story is flat - it leaves you emotionless the entire movie. The dialogue is extremely weak and predictable boasting lines of \"\"Woah, you totally freaked me out\"\" and \"\"I was wondering if you'd uh...if you'd like to..uh, would you come to the cabin with me?\"\". It makes me want to rip out all my hair, one strand at a time and feed it to myself.<br /><br />THE CHARACTERS: HOLY CRAP!!!! Some have described the characters as flat, I want to take it one step further and say that they actually have a reverse character arch.. They actually start working on a parallel universe and almost start acting backwards...<br /><br />THE ACTORS: Worse than the characters are the actors. They take already poor written characters and add in terrible high school drama acting. The \"\"Woah you totally freaked me out\"\" was said so monotone and slow - like it was dumbed down. I could complain for hours on the actors alone.<br /><br />TECHNICAL: LIGHTING: An eight year old would be disappointed with lighting on this movie. Too shadowy in areas, too bleached in others. The director shouldn't use light as an emotion until he learns how to light a basic scene properly. Baby Steps! SOUND: How many sound guys does it take to make a really shotty sounding movie? 9. With that many sound guys this should sound amazing but quite the opposite has occured. There is one scene in particular that really sticks out, these guys are driving in a car and the sound of the car changes with every camera angle....WEAK! CAMERA: Learn to use it.<br /><br />Anyway, I'm running out of complaining space.....rent it - I dare you...Rent it and learn from it...give it a 1 rating..it deserves it.<br /><br />Signing off... Amanda Christmas\"\r\n",
            "1\t\"My all-time favorite movie! I have seen many movies, but this one beats them all! Excelent acting, wonderful story. You will, as a \"\"normal\"\" caring person start to love George. Altough he is an actor, he is also himself and a very lovable person. And maby most important thing: you will learn to respect & look different to people with Down Syndrome.\"\r\n",
            "1\tWonderful film, one of the best horror films of the 70s. She is realistic settings and atmospheres. As usual it was inevitable the usual negative comments. I have noticed that most horror films of a certain period many times fail to reach even sufficiency. Obviously because most horror movies are old and must be denigrati, is like a mental mechanism that moves the minds of the potential of music critics here.<br /><br />Before you read the review already knew what was the final judgment. In the film a good gift because 10 is really well done. Raines reads quite well and the film as a way in which it was produced reminds me a lot of Kubrick films. He really impression. Excellent film really. I consider a film anthology of years'70.\r\n",
            "0\tand shot in Vancouver with the 'mountains' of the low country of South Carolina visible in the background. For heaven's sake, they should have reset the location. There are no coastal mountains in South Carolina. Period.<br /><br />Lame visuals. They should have been beautiful. And the story limped along.<br /><br />I really don't understand why it was such a hit as a book, although I have to admit it's one I haven't read as yet. Usually I read the book and give the film a miss. There was nothing in this movie that made me want to buy the book, or even borrow it from the library.<br /><br />Verdict: The Mermaid Chair seemed pretty shallow to moi.\r\n",
            "1\tThis is the best dub I've ever heard by Disney, as well as the best adaptation since the biggest abuse ever on soundtrack, themes, characters, dialogues in Kiki Delivery Service. Urrrghhh<br /><br />This one has different atmosphere, especially the deviation from the common heroine. This one has both hero and heroine (although I don't really endorse the use of hero & heroine here, since Miyazaki is out from the stereotype & common theme). As usual, after being introduced by Spirited away, amazed by Mononoke, troubled by Grave of Fireflies, and deeply touched by Majo no Takkyuubin , this one start with a bit doubt in my part. Wondering if this will be the first Ghibi's dud. Well, in the end just like Only Yesterday and Whisper of the Heart, I ended up giving 10 rating. I'd give 9.8 rating, but the additional 0.2 is there to share the good feeling by encouraging people to see the movie.<br /><br />SPOILER Somehow I see this as a sad movie, people die in this one, the lonely robot, the abandoned place, and it ends with destruction. It is as if mankind really can't live with too much power. The collapsing scene gave me patches of Metropolis ending. It's just sad somehow. The plot is apparent in most reviews and the soundtrack rules as well (as always). Joe Hisaishi really belongs to Uematsu, Kanno, Williams caliber.People who can brings a movie, a game, an event to life, even to be a lingering moments by astounding composition.<br /><br />This is a feel good movie that used to be part of US cinema in the classic days (It's a Wonderful Life etc etc). Well, things change....\r\n",
            "1\t\"Linking story: another first-time viewing for me and, again, this is one of the most popular of the Amicus anthologies - and it's easy to see why, though I realize how the film's rather meaningless title could be misleading for some; I certainly fancied director Peter Duffell's choice - DEATH AND THE MAIDEN (which, incidentally, is a classical piece by Schubert that is heard in the film during the Peter Cushing episode) - a great deal more. Though the linking device itself is not all that great, the episodes are all equally compelling and enjoyable. Production values come off as very respectable indeed for the budget Duffell had to work with. The latter infuses the film with a great deal of style which is not so common with this type of film and, frankly, it makes one regret the fact that he wished to distance himself from the genre (though more so as not to be typecast rather than because he felt it was beneath him).<br /><br />Now to the individual stories themselves: <br /><br />\"\"Method For Murder\"\": the opening segment does not offer any real surprises but, to make up for this, it's quietly suspenseful and appropriately creepy at times (Tom Adams' 'fictitious' villain looking like the long-lost brother of Boris Karloff from THE OLD DARK HOUSE [1932]); also, it ends with a satisfactory DIABOLIQUE-type twist, and features a fairly intense role for Denholm Elliott in the lead. That's all we need out of it, really.<br /><br />\"\"Waxworks\"\": for the second story we are introduced to a curiously romantic mood which is quite unusual for this type of film; Peter Cushing and Joss Ackland are both excellent (as well as impeccably dressed) in their roles of two jilted lovers of a woman who continues to obsess them even after such a long time, and whose friendly rivalry can only lead them blindly and inexorably to a fate that is literally worse than death; an ominous hallucination scene with Peter Cushing is quite well done in view of the limited resources at hand, and Ackland's inexplicable inability - or unwillingness - to leave town somewhat recalls the house-trapped aristocrats of Bunuel's THE EXTERMINATING ANGEL (1962).<br /><br />\"\"Sweets To The Sweet\"\": this is perhaps the finest episode of all - with his ambiguous role here, Christopher Lee continues to demonstrate his versatility and he is matched by an understanding Nyree Dawn Porter and the deceptive innocence of Chloe Franks (who appears as Lee's daughter). The film's treatment of the occult here is both subtle and mature, culminating in a powerful and extremely chilling 'curtain'. Trivia Note: Chloe Franks appears as a grown-up in the featurette included on the disc, and when I saw her I felt an immediate familiarity with her face but couldn't quite put my finger on it. Later, on reading her filmography, it was revealed to me that she had played one of the leads in the long-running stage adaptation of Agatha Christie's \"\"The Mousetrap\"\" in London's West End, which my brother and I were fortunate enough to catch while we there on holiday in the Summer of 2002! Needless to say, we had no idea then that she had once created such a delicate - and delicious - portrayal in sheer evil, mainly by virtue of her peculiar look and a devilish smile!! <br /><br />\"\"The Cloak\"\": a wacky but oddly reverent vampire tale (that still manages to debunk many of the myths attached to the subgenre, while inventing some new ones!) which takes in some wonderful digs at exploitation cinema and, at one point, Christopher Lee himself!; Jon Pertwee is marvelous as the campy horror star who gets more than he bargained for when he attempts to bring a measure of authenticity to his work; Ingrid Pitt sends up her image nicely though her role is somewhat subsidiary to the proceedings; Geoffrey Bayldon (made up to look like Ernest Thesiger) also has a memorably quirky bit; the 'silent-cinema' style of the ending was a pretty audacious one to pull on an audience, I suppose - and, while some of the humor comes off as heavy-handed a' la THE FEARLESS VAMPIRE KILLERS (1967) or THEATRE OF BLOOD (1973), it's also rather infectious and certainly ends the film on a high (and highly unusual) note! <br /><br />Video and audio quality are relatively satisfactory, considering I had no other version to compare it with; the main culprit is some noticeable print damage but this is never so nasty as to affect one's enjoyment of the film. As for the extras, beginning with the Audio Commentary: frankly, this is one of the finest chats about a genre film that I can remember listening to; Jonathan Rigby gets to butt in with his opinion more than is usual for a moderator but his effort certainly allows director Peter Duffell to touch on every aspect of the production (whereas with some other films, you're left rather expecting there to be more!) and, as such, it's an extremely pleasant track that complements the main feature very nicely indeed. The featurette \"\"A-Rated Horror Film\"\" is a worthwhile effort with Peter Duffell again at center-stage but this time backed up with valid, if all-too-brief, contributions from producer Max J. Rosenberg and stars Chloe Franks, Ingrid Pitt and Geoffrey Bayldon. We also get film notes, reviews, bios and a poster/stills gallery which, again, are wonderfully assembled (with the contemporary reviews being something of a novelty - and a welcome one at that).\"\r\n"
          ]
        }
      ],
      "source": [
        "!head train.tsv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5R9e3pctHIV2",
        "outputId": "3a756a9d-06cf-4a38-f38c-b94bc9af2647"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train size = 25000\n",
            "Test size = 25000\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "train_df = pd.read_csv(\"train.tsv\", delimiter=\"\\t\")\n",
        "test_df = pd.read_csv(\"test.tsv\", delimiter=\"\\t\")\n",
        "\n",
        "print('Train size = {}'.format(len(train_df)))\n",
        "print('Test size = {}'.format(len(test_df)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_Ega6Z0w2XV6",
        "outputId": "90a3a64a-7807-4a59-a568-cfadafcf8add"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   is_positive                                             review\n",
              "0            0  Dreamgirls, despite its fistful of Tony wins i...\n",
              "1            0  This show comes up with interesting locations ...\n",
              "2            1  I simply love this movie. I also love the Ramo...\n",
              "3            0  Spoilers ahead if you want to call them that.....\n",
              "4            1  My all-time favorite movie! I have seen many m..."
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8ff356df-b291-4fc7-b88b-6925856f778c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>is_positive</th>\n",
              "      <th>review</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Dreamgirls, despite its fistful of Tony wins i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>This show comes up with interesting locations ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>I simply love this movie. I also love the Ramo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>Spoilers ahead if you want to call them that.....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>My all-time favorite movie! I have seen many m...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8ff356df-b291-4fc7-b88b-6925856f778c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8ff356df-b291-4fc7-b88b-6925856f778c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8ff356df-b291-4fc7-b88b-6925856f778c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-7794c8f1-ed9f-4367-a28d-30b6688dde18\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-7794c8f1-ed9f-4367-a28d-30b6688dde18')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-7794c8f1-ed9f-4367-a28d-30b6688dde18 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 25000,\n  \"fields\": [\n    {\n      \"column\": \"is_positive\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 24904,\n        \"samples\": [\n          \"1933 seemed to be a great year for satires (\\\"Duck Soup\\\" for instance) and this one fits in well even though it is about the obsession with contract bridge. The tone is like a humorous piece from The New Yorker, appropriate, since the film begins with the \\\"Goings On About Town\\\" page of that magazine. The only thing odd is the casting. Made a few years later William Powell and Myrna Loy would have been perfect. However, after 1934, you wouldn't have had adultery handled in such a sophisticated fashion, the young and beautiful Loretta Young in some shear and slinky outfits, or a group of prostitutes listening to a bridge contest on radio. Even if you know nothing about bridge, you may still want to check out a rare example of Hollywood satire.\",\n          \"This show was laughably bad. The writing sucked, the dialog sucked. The guy who played Craig couldn't act his way out of a paper sack. Being it was on Thursday night, this was definitely great to watch with some beers. Cool music, bad acting, poor writing, all came together for my entertainment.<br /><br />It was a drama/unintentional comedy. I don't care what happened to any of the characters, they were all boring and stupid. The first five episodes were the worst, since they couldn't reveal who the victim was, they had to write the dialog around it, which was terrible. I mean, the eulogy at the funeral was ridiculous. Actually, all the scenes that occurred in the present were utterly horrible.<br /><br />So, let's review. Everything happening in present time sucked. The flashback scenes, only the writing, dialog and Craig's acting sucked. The music ruled though.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "train_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oHbJCPPwhjp"
      },
      "source": [
        "Посмотрите глазами на тексты? Какие есть зацепки, как определить, что это за сентимент?\n",
        "\n",
        "Самое простое, как всегда - найти ключевые слова."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B-AH3H5Bo_qF"
      },
      "outputs": [],
      "source": [
        "positive_words = (\n",
        "    \"happy\",\n",
        "    \"joyful\",\n",
        "    \"excited\",\n",
        "    \"awesome\",\n",
        "    \"fantastic\",\n",
        "    \"amazing\",\n",
        "    \"wonderful\",\n",
        "    \"great\",\n",
        "    \"excellent\",\n",
        "    \"terrific\",\n",
        "    \"brilliant\",\n",
        "    \"beautiful\",\n",
        "    \"lovely\",\n",
        "    \"delightful\",\n",
        "    \"positive\",\n",
        "    \"uplifting\",\n",
        "    \"inspiring\",\n",
        "    \"thrilling\",\n",
        "    \"vibrant\",\n",
        "    \"satisfying\"\n",
        "    )\n",
        "negative_words = (\n",
        "    \"sad\",\n",
        "    \"depressed\",\n",
        "    \"unhappy\",\n",
        "    \"miserable\",\n",
        "    \"awful\",\n",
        "    \"terrible\",\n",
        "    \"horrible\",\n",
        "    \"bad\",\n",
        "    \"negative\",\n",
        "    \"disappointing\",\n",
        "    \"frustrating\",\n",
        "    \"annoying\",\n",
        "    \"stressful\",\n",
        "    \"difficult\",\n",
        "    \"unpleasant\",\n",
        "    \"down\",\n",
        "    \"disheartening\",\n",
        "    \"grim\",\n",
        "    \"dismal\",\n",
        "    \"gloomy\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DywUCyMLr_TD",
        "outputId": "60df7540-96c1-49ff-f233-0343b0c8d2c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 71.19%\n"
          ]
        }
      ],
      "source": [
        "#@title Начинаем классифицировать! { vertical-output: true, display-mode: \"form\" }\n",
        "positive_words = positive_words + ('love', 'great', 'best', 'wonderful') #@param {type:\"raw\"}\n",
        "negative_words = negative_words + ('worst', 'awful', '1/10', 'crap') #@param {type:\"raw\"}\n",
        "\n",
        "positives_count = test_df.review.apply(lambda text: sum(word in text for word in positive_words))\n",
        "negatives_count = test_df.review.apply(lambda text: sum(word in text for word in negative_words))\n",
        "is_positive = positives_count > negatives_count\n",
        "correct_count = (is_positive == test_df.is_positive).values.sum()\n",
        "\n",
        "accuracy = correct_count / len(test_df)\n",
        "\n",
        "print('Test accuracy = {:.2%}'.format(accuracy))\n",
        "# if accuracy > 0.71:\n",
        "#     from IPython.display import Image, display\n",
        "#     display(Image('https://s3.amazonaws.com/achgen360/t/rmmoZsub.png', width=500))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oo8HRABxv1kW"
      },
      "source": [
        "**Задание** Придумайте хорошие ключевые слова или фразы и наберите хотя бы 71% точности на тесте (и не забудьте посмотреть на код классификации!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaIrBClMUHZB"
      },
      "source": [
        "**Задание** Кому-нибудь нравятся эти `<br /><br />`? Лично мне - нет. Напишите регулярку, которая будет их удалять"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09OkUmtde6ny",
        "outputId": "d648280b-0a7c-4ea6-8fa8-4f5c503fdd05"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spoilers ahead if you want to call them that...<br /><br />I would almost recommend this film just so people can truly see a 1/10. Where to begin, we'll start from the top...<br /><br />THE STORY: Don't believe the premise - the movie has nothing to do with abandoned cars, and people finially understanding what the mysterious happenings are. It's a draub, basic, go to cabin movie with no intensity or \"effort\".<br /><br />THE SCREENPLAY: I usually give credit to indie screenwriters, it's hard work when you are starting out...but this is crap. The story is flat - it leaves you emotionless the entire movie. The dialogue is extremely weak and predictable boasting lines of \"Woah, you totally freaked me out\" and \"I was wondering if you'd uh...if you'd like to..uh, would you come to the cabin with me?\". It makes me want to rip out all my hair, one strand at a time and feed it to myself.<br /><br />THE CHARACTERS: HOLY CRAP!!!! Some have described the characters as flat, I want to take it one step further and say that they actually have a reverse character arch.. They actually start working on a parallel universe and almost start acting backwards...<br /><br />THE ACTORS: Worse than the characters are the actors. They take already poor written characters and add in terrible high school drama acting. The \"Woah you totally freaked me out\" was said so monotone and slow - like it was dumbed down. I could complain for hours on the actors alone.<br /><br />TECHNICAL: LIGHTING: An eight year old would be disappointed with lighting on this movie. Too shadowy in areas, too bleached in others. The director shouldn't use light as an emotion until he learns how to light a basic scene properly. Baby Steps! SOUND: How many sound guys does it take to make a really shotty sounding movie? 9. With that many sound guys this should sound amazing but quite the opposite has occured. There is one scene in particular that really sticks out, these guys are driving in a car and the sound of the car changes with every camera angle....WEAK! CAMERA: Learn to use it.<br /><br />Anyway, I'm running out of complaining space.....rent it - I dare you...Rent it and learn from it...give it a 1 rating..it deserves it.<br /><br />Signing off... Amanda Christmas\n",
            "Spoilers ahead if you want to call them that...  I would almost recommend this film just so people can truly see a 1/10. Where to begin, we'll start from the top...  THE STORY: Don't believe the premise - the movie has nothing to do with abandoned cars, and people finially understanding what the mysterious happenings are. It's a draub, basic, go to cabin movie with no intensity or \"effort\".  THE SCREENPLAY: I usually give credit to indie screenwriters, it's hard work when you are starting out...but this is crap. The story is flat - it leaves you emotionless the entire movie. The dialogue is extremely weak and predictable boasting lines of \"Woah, you totally freaked me out\" and \"I was wondering if you'd uh...if you'd like to..uh, would you come to the cabin with me?\". It makes me want to rip out all my hair, one strand at a time and feed it to myself.  THE CHARACTERS: HOLY CRAP!!!! Some have described the characters as flat, I want to take it one step further and say that they actually have a reverse character arch.. They actually start working on a parallel universe and almost start acting backwards...  THE ACTORS: Worse than the characters are the actors. They take already poor written characters and add in terrible high school drama acting. The \"Woah you totally freaked me out\" was said so monotone and slow - like it was dumbed down. I could complain for hours on the actors alone.  TECHNICAL: LIGHTING: An eight year old would be disappointed with lighting on this movie. Too shadowy in areas, too bleached in others. The director shouldn't use light as an emotion until he learns how to light a basic scene properly. Baby Steps! SOUND: How many sound guys does it take to make a really shotty sounding movie? 9. With that many sound guys this should sound amazing but quite the opposite has occured. There is one scene in particular that really sticks out, these guys are driving in a car and the sound of the car changes with every camera angle....WEAK! CAMERA: Learn to use it.  Anyway, I'm running out of complaining space.....rent it - I dare you...Rent it and learn from it...give it a 1 rating..it deserves it.  Signing off... Amanda Christmas\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "pattern = re.compile('<br />')\n",
        "\n",
        "print(train_df['review'].iloc[3])\n",
        "print(pattern.subn(' ', train_df['review'].iloc[3])[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vO6D9NuMi4II"
      },
      "source": [
        "Применим ее:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "7LTwQqs_hD-K"
      },
      "outputs": [],
      "source": [
        "train_df['review'] = train_df['review'].apply(lambda text: pattern.subn(' ', text)[0])\n",
        "test_df['review'] = test_df['review'].apply(lambda text: pattern.subn(' ', text)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGxzf4oXmXqw"
      },
      "source": [
        "Пора переходить к машинке!\n",
        "\n",
        "Как будем представлять текст? Проще всего - мешком слов.\n",
        "\n",
        "Заведём большой-большой словарь - список всех слов в обучающей выборке. Тогда каждое предложение можно представить в виде вектора, в котором будет записано, сколько раз встретилось каждое из возможных слов:\n",
        "\n",
        "![bow](https://raw.githubusercontent.com/DanAnastasyev/DeepNLP-Course/master/Week%2001/Images/BOW.png)\n",
        "\n",
        "Простой и приятный способ сделать это - запихнуть тексты в `CountVectorizer`.\n",
        "\n",
        "Он имеет такую сигнатуру:\n",
        "\n",
        "```python\n",
        "CountVectorizer(input='content', encoding='utf-8', decode_error='strict', strip_accents=None, lowercase=True, preprocessor=None, tokenizer=None, stop_words=None, token_pattern=r'(?u)\\b\\w\\w+\\b', ngram_range=(1, 1), analyzer='word', max_df=1.0, min_df=1, max_features=None, vocabulary=None, binary=False, dtype=<class ‘numpy.int64'>)\n",
        "```\n",
        "\n",
        "Для начала обратим внимание на параметры `lowercase=True` и `max_df=1.0, min_df=1, max_features=None` - они про то, что по умолчанию все слова будут приводиться к нижнему регистру и в словарь попадут все слова, встречавшиеся в текстах.\n",
        "\n",
        "При желании можно было бы убрать слишком редкие или слишком частотные слова - пока не будем этого делать.\n",
        "\n",
        "Посмотрим на простом примере, как он будет работать:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Odnum4iyGDr",
        "outputId": "b37ea27a-1e4b-45ac-85e0-885667d695f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 1 1 1]\n",
            " [1 0 1 1 1]]\n",
            "['awful' 'excellent' 'movie' 'the' 'was']\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "dummy_data = ['The movie was excellent',\n",
        "              'the movie was awful']\n",
        "\n",
        "dummy_matrix = vectorizer.fit_transform(dummy_data)\n",
        "\n",
        "print(dummy_matrix.toarray())\n",
        "print(vectorizer.get_feature_names_out())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3zC1ItWybVc"
      },
      "source": [
        "*Как именно vectorizer определяет границы слов? Обратите внимание на параметр `token_pattern=r'(?u)\\b\\w\\w+\\b'` - как он будет работать?*\n",
        "\n",
        "Запустим его на реальных данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 75
        },
        "id": "Ccd2gaCdQq2W",
        "outputId": "07ee4bb6-4893-4d74-809e-30b1b4370d49"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer()"
            ],
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(train_df['review'].values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_JC9n6C0bFR"
      },
      "source": [
        "Посмотрим на слова, попавшие в словарь:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "stV4ICO3mKsf",
        "outputId": "924f3248-fd36-4707-d428-7e0844f2af1f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['00', '000', '0000000000001', '00001', '00015', '000s', '001',\n",
              "       '003830', '006', '007'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "vectorizer.get_feature_names_out()[:5]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oesbuQ9g0krj"
      },
      "source": [
        "Попробуем кого-нибудь таки сконвертировать"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUWtDWcp0g7U",
        "outputId": "a971dd25-8727-424a-8bc7-1ccdebbfaef4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<1x74849 sparse matrix of type '<class 'numpy.int64'>'\n",
              "\twith 206 stored elements in Compressed Sparse Row format>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "vectorizer.transform([train_df['review'].iloc[3]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZCRef3pyCRC"
      },
      "source": [
        "То, что и хотели - вектор с bow (т.е. bag-of-words) представлением исходного текста.\n",
        "\n",
        "И чем эта информация может помочь? Ну, всё тем же - какие-то слова носят положительный окрас, какие-то - отрицательный. Большинство вообще нейтральный, да.\n",
        "\n",
        "![bow with weights](https://github.com/DanAnastasyev/DeepNLP-Course/raw/master/Week%2001/Images/BOW_weights.png)\n",
        "\n",
        "Хочется, наверное, подобрать коэффициенты, которые будут определять уровень окраса, да? Подбирать нужно по обучающей выборке, а не как мы перед этим делали.\n",
        "\n",
        "Например, для выборки\n",
        "```\n",
        "1   The movie was excellent\n",
        "0   the movie was awful\n",
        "```\n",
        "легко подобрать коэффициенты на глазок: что-нибудь вроде `+1` для `excellent`,  `-1` для `awful` и по нулям всем остальным.\n",
        "\n",
        "Построим линейную модель, которая станет этим заниматься. Она будет учиться строить разделяющую гиперплоскость в пространстве bow-векторов.\n",
        "\n",
        "Проверим, как справится логистическая регрессия с нашей супер-выборкой из пары предложений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6WVgK4LtUn2",
        "outputId": "82f4becc-349d-4e39-ebd0-362c120808e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['awful' 'excellent' 'movie' 'the' 'was']\n",
            "[[-0.40104279  0.40104279  0.          0.          0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "dummy_data = ['The movie was excellent',\n",
        "              'the movie was awful']\n",
        "dummy_labels = [1, 0]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(dummy_data, dummy_labels)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(classifier.coef_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C9Y-kq-tv-XY"
      },
      "source": [
        "Получилось что надо.\n",
        "\n",
        "Запустим теперь её на реальных данных."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "kvxHIIbSiUXq",
        "outputId": "7ba70c2c-a5f7-44ed-8fb1-41f2bf7c2dfb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('vectorizer', CountVectorizer()),\n",
              "                ('classifier', LogisticRegression())])"
            ],
            "text/html": [
              "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
              "                (&#x27;classifier&#x27;, LogisticRegression())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;vectorizer&#x27;, CountVectorizer()),\n",
              "                (&#x27;classifier&#x27;, LogisticRegression())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "model.fit(train_df['review'], train_df['is_positive'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-d3BBV_uUu-O",
        "outputId": "36e9376b-f76b-4de6-9f84-9c50b6c4e22b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 86.51%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def eval_model(model, test_df):\n",
        "    preds = model.predict(test_df['review'])\n",
        "    print('Test accuracy = {:.2%}'.format(accuracy_score(test_df['is_positive'], preds)))\n",
        "\n",
        "eval_model(model, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pnmmz3u71ctO"
      },
      "source": [
        "Прогресс!\n",
        "\n",
        "Хочется как-то посмотреть, что заинтересовало классификатор. К счастью, сделать это совсем просто:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "8W1Ngl-aVuYx",
        "outputId": "dd7e1d2c-dacc-4850-8689-199820bffbd1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "            \n",
              "                \n",
              "                \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=1\n",
              "    \n",
              "</b>\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature weights. Note that weights do not account for feature value scales, so if feature values have different scales, features with highest weights might not be the most important.\">\n",
              "                    Weight<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 84.43%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.863\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x54378\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 84.98%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.771\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x73498\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 84.98%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 37698 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 85.21%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 37142 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 85.21%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.731\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x23564\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 85.20%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.733\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x37919\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 83.68%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.993\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x42368\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 82.75%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.156\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x37435\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 81.97%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.298\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x73700\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 81.62%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.362\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x50821\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.18%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.631\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x72229\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -2.664\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x18642\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "            \n",
              "        \n",
              "\n",
              "        \n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "import eli5\n",
        "eli5.show_weights(classifier, vec=vectorizer, top=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VgCE9tDk-aO"
      },
      "source": [
        "Посмотрим на конкретные примеры его работы:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "v-4sVWBOWJpo",
        "outputId": "b8224318-3d8a-4af3-96e1-d8ad4e946273"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=positive\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>1.000</b>, score <b>18.990</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.980\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x9114\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 83.01%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.568\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x44172\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 83.85%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.459\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x22056\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.19%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.167\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x67534\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.79%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.978\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x8583\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.18%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.934\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x31095\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.39%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.911\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x66339\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.46%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.903\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x12676\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.67%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.880\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x65067\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 88.67%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 97 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 82.69%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 91 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 82.69%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.611\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x35749\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"opacity: 0.80\">this is both an entertaining and a touching version of the classic tale, also quite intelligent, not of the &#x27;me tarzan, you jane&#x27; school at all.  it&#x27;s the famous story of a child reared to manhood in the jungle by apes. a titled british couple (the wife pregnant) is stranded in the african wilds after a shipwreck. after the parents&#x27; deaths, the baby is raised in the jungle by apes. twenty years later, this young man (i.e. tarzan) rescues a wounded belgian explorer, nursing him back to health. the belgian discovers evidence that his rescuer is the young lord greystoke and returns him to his rightful estate in scotland, where he must adjust to civilized society.   the movie is sort of divided into two parts. in the first half, we see tarzan in his jungle environment. not being an expert, i am unaware as to the realism of its depiction of ape community life, but it is certainly entertaining. for me, the more moving section is the second half, when tarzan must meet his real family, develop language skills, and adjust to aristocratic british society, all the while wooing jane (andie macdowell). he is portrayed as a &#x27;noble savage&#x27;, whether in the wild or in elegant edwardian parlors. by contrast, the upper crust is depicted as often far more barbaric than the jungle tarzan left.  christopher lambert is fantastic in his sympathetic portrayal of tarzan in both the jungle and civilized environments. he conveys a real sense of his confusion and conflict, torn as he is between the two very different worlds, his original ape family and his new human one. sir ralph richardson, one of the old british legends, is brilliant as always in the role of tarzan&#x27;s grandfather, the sixth earl of greystoke.   the film focuses more on tarzan&#x27;s struggles in adapting to civilization and his inner conflict than on his jungle exploits. this unusual take on the old classic makes it both the typical dramatic adventure but also, above all, a moving personal story. i wasn&#x27;t surprised to note here that its director is the same individual, hugh hudson, who also directed chariots of fire, another brilliant movie.</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer,\n",
        "                     targets=['positive'], target_names=['negative', 'positive'], top=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "AoZhtlYlW-xG",
        "outputId": "e58a4633-16a4-4932-ea53-6a588bd03971"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=positive\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.000</b>, score <b>-10.950</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 38 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 88.60%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 38 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 88.60%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.668\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x56749\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 87.86%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.731\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x21041\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 87.75%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.740\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x50428\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 84.63%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.024\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x36145\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 83.95%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.089\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x66150\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 83.69%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.115\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x60641\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 83.09%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.174\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x55773\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 82.54%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.229\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x3513\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.61%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.427\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x57668\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.492\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x11782\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"opacity: 0.80\">this movie is terrible, it was so difficult to believe that katie became a heartfelt teenager with the power to save the pity chinese people, the movie didn&#x27;t show any convincing argument to prove that. and the rest of the plot didn&#x27;t make any effort to show us more than a cheap common sense...   the plot is ridiculous and the only thing we can extract from it is that it demonstrate how arrogant a human can be. katie must have inherited her arrogance from her mother, the most annoying character i have seen for a long time.   the acting and scenery were ok, but the plot ruins everything, full of cheap clichés and hypocritical scenes, i expect not to see this movie again in my life. skip this one!</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "print('Positive' if test_df['is_positive'].iloc[6] else 'Negative')\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[6], vec=vectorizer,\n",
        "                     targets=['positive'], target_names=['negative', 'positive'], top=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNNUqZvplhAC"
      },
      "source": [
        "Посмотрим на примеры неправильной классификации, наконец:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "yf9ZzS8fXKFm",
        "outputId": "619aa822-8a9d-4b09-eb6e-08cf480f4f80"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=positive\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.150</b>, score <b>-1.735</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 81.98%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.256\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x26333\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 84.40%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.022\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x58828\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 84.45%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +1.018\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x34683\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 86.68%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.815\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x28545\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.49%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.746\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x39486\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 87.49%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 53 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 86.99%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 67 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 86.99%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.789\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x5502\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 86.27%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.852\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x13339\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 84.96%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.970\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x56835\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 82.82%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.174\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x55773\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -1.458\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x46074\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"opacity: 0.80\">when i first heard about &quot;greek,&quot; i figured i would watch it because it sounded ridiculous. another of abc family&#x27;s so-bad-they&#x27;re-almost-good shows. but tuning in with a friend from college found us both enjoying the pilot episode a lot more than we had expected.  as a member of a greek society, i can say that a lot of the stereotypes that are brought up here are ones that come up almost every time someone starts talking about the sororities and fraternities on a campus. and are also very fun to play with just on are own, let alone to watch on a tv screen. the opening scene harkened to an only-slightly-dramatized version of preparing for an actual formal rush in some sororities and it continued on from there.  this isn&#x27;t a show for over-sensitive greeks. if you get offended even at jokes about things that aren&#x27;t-so-great about greek life, then you&#x27;ll spend the entire first episode, and probably many other, cringing and yelling. but everyone else should have a ton of fun watching it. it&#x27;s nothing new, but when it comes to college, nothing ever is.</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "preds = model.predict(test_df['review'])\n",
        "incorrect_pred_index = np.random.choice(np.where(preds != test_df['is_positive'])[0])\n",
        "\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[incorrect_pred_index],\n",
        "                     vec=vectorizer, targets=['positive'], target_names=['negative', 'positive'], top=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbFXKNrngP46"
      },
      "source": [
        "## Придумываем новые признаки"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dz7GSFzIlv4V"
      },
      "source": [
        "### Tf-idf\n",
        "\n",
        "Сейчас мы на все слова смотрим с одинаковым весом - хотя какие-то из них более редкие, какие-то более частые, и эта частотность - полезная, вообще говоря, информация.\n",
        "\n",
        "Самый простой способ добавить статистическую информацию о частотностях - сделать *tf-idf* взвешивание:\n",
        "\n",
        "$$\\text{tf-idf}(t, d) = \\text{tf}(t, d) \\times \\text{idf}(t)$$\n",
        "\n",
        "*tf* - term-frequency - частотность слова `t` в конкретном документе `d` (рецензии в нашем случае). Это ровно то, что мы уже считали.\n",
        "\n",
        "*idf* - inverse document-frequency - коэффициент, который тем больше, чем в меньшем числе документов встречалось данное слово. Считается как-нибудь так:\n",
        "$$\\text{idf}(t) = \\text{log}\\frac{1 + n_d}{1 + n_{d(t)}} + 1$$\n",
        "где $n_d$ - число всех документов, а $n_{d(t)}$ - число документов со словом `t`.\n",
        "\n",
        "Использовать его просто - нужно заменить `CountVectorizer` на `TfidfVectorizer`.\n",
        "\n",
        "**Задание** Попробуйте запустить `TfidfVectorizer`. Посмотрите на ошибки, которые он научился исправлять, и на ошибки, которые он начал делать - по сравнению с `CountVectorizer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3DjjiJglvT3",
        "outputId": "2e57563c-5e9c-4d1c-f62d-9a56e1b93bab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 88.28%\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "vectorizer = TfidfVectorizer()\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(train_df['review'], train_df['is_positive'])\n",
        "\n",
        "eval_model(model, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xe_CJxQ0tFP9"
      },
      "source": [
        "### N-граммы слов\n",
        "\n",
        "До сих пор мы смотрели на тексты как на мешок слов - но очевидно, что есть разница между `good movie` и `not good movie`.\n",
        "\n",
        "Добавим информацию (хоть какую-то) о последовательностях слов - будем извлекать еще и биграммы слов.\n",
        "\n",
        "В Vectorizer'ах для этого есть параметр `ngram_range=(n_1, n_2)` - он говорит, что нужны n_1-...n_2-граммы.\n",
        "\n",
        "**Задание** Попробуйте увеличенный range и поинтерпретируйте полученный результат."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RDpdrT0HuKYN",
        "outputId": "922e0c11-f61b-40e0-f642-5067da65cbe7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 88.64%\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(train_df['review'], train_df['is_positive'])\n",
        "\n",
        "eval_model(model, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrBoThj6wl2F"
      },
      "source": [
        "### N-граммы символов\n",
        "\n",
        "Символьные n-граммы дают простой способ выучить полезные корни и суффиксы, не связываясь с этой вашей лингвистикой - только статистика, только хардкор.\n",
        "\n",
        "Например, слово `badass` мы можем представить в виде такой последовательности триграмм:\n",
        "\n",
        "`##b #ba bad ada das ass ss# s##`\n",
        "\n",
        "So interpretable, неправда ли?\n",
        "\n",
        "Реализовать это дело всё так же просто - нужно поставить `analyzer='char'` в вашем любимом Vectorizer'е и выбрать размер `ngram_range`.\n",
        "\n",
        "**Задание** Запилите классификатор на n-граммах символов и визуализируйте его."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QFaWmUrGyY3n",
        "outputId": "5711f1fc-18b8-473c-a2a3-347b732e510f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 87.86%\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(train_df['review'], train_df['is_positive'])\n",
        "\n",
        "eval_model(model, test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 483
        },
        "id": "9E1E1Bn8yvhq",
        "outputId": "7a220955-08d5-4526-8181-236af7726e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <style>\n",
              "    table.eli5-weights tr:hover {\n",
              "        filter: brightness(85%);\n",
              "    }\n",
              "</style>\n",
              "\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "        \n",
              "\n",
              "    \n",
              "\n",
              "        \n",
              "\n",
              "        \n",
              "    \n",
              "        \n",
              "        \n",
              "    \n",
              "        <p style=\"margin-bottom: 0.5em; margin-top: 0em\">\n",
              "            <b>\n",
              "    \n",
              "        y=positive\n",
              "    \n",
              "</b>\n",
              "\n",
              "    \n",
              "    (probability <b>0.962</b>, score <b>3.245</b>)\n",
              "\n",
              "top features\n",
              "        </p>\n",
              "    \n",
              "    <table class=\"eli5-weights\"\n",
              "           style=\"border-collapse: collapse; border: none; margin-top: 0em; table-layout: auto; margin-bottom: 2em;\">\n",
              "        <thead>\n",
              "        <tr style=\"border: none;\">\n",
              "            \n",
              "                <th style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\" title=\"Feature contribution already accounts for the feature value (for linear models, contribution = weight * feature value), and the sum of feature contributions is equal to the score or, for some classifiers, to the probability. Feature values are shown if &quot;show_feature_values&quot; is True.\">\n",
              "                    Contribution<sup>?</sup>\n",
              "                </th>\n",
              "            \n",
              "            <th style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">Feature</th>\n",
              "            \n",
              "        </tr>\n",
              "        </thead>\n",
              "        <tbody>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.00%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.113\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x9227\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 80.58%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.108\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x1329\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 83.71%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.084\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x4246\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 85.05%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.075\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x17881\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 85.32%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        +0.073\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x6343\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(120, 100.00%, 85.32%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 1999 more positive &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 85.09%); border: none;\">\n",
              "                <td colspan=\"2\" style=\"padding: 0 0.5em 0 0.5em; text-align: center; border: none; white-space: nowrap;\">\n",
              "                    <i>&hellip; 1509 more negative &hellip;</i>\n",
              "                </td>\n",
              "            </tr>\n",
              "        \n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 85.09%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.074\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x419\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 84.39%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.079\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x5751\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 84.20%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.081\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x2312\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 83.58%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.085\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x14238\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "            <tr style=\"background-color: hsl(0, 100.00%, 81.87%); border: none;\">\n",
              "    <td style=\"padding: 0 1em 0 0.5em; text-align: right; border: none;\">\n",
              "        -0.098\n",
              "    </td>\n",
              "    <td style=\"padding: 0 0.5em 0 0.5em; text-align: left; border: none;\">\n",
              "        x10691\n",
              "    </td>\n",
              "    \n",
              "</tr>\n",
              "        \n",
              "\n",
              "        </tbody>\n",
              "    </table>\n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n",
              "    <p style=\"margin-bottom: 2.5em; margin-top:-0.5em;\">\n",
              "        <span style=\"opacity: 0.80\">this is both an entertaining and a touching version of the classic tale, also quite intelligent, not of the &#x27;me tarzan, you jane&#x27; school at all. it&#x27;s the famous story of a child reared to manhood in the jungle by apes. a titled british couple (the wife pregnant) is stranded in the african wilds after a shipwreck. after the parents&#x27; deaths, the baby is raised in the jungle by apes. twenty years later, this young man (i.e. tarzan) rescues a wounded belgian explorer, nursing him back to health. the belgian discovers evidence that his rescuer is the young lord greystoke and returns him to his rightful estate in scotland, where he must adjust to civilized society. the movie is sort of divided into two parts. in the first half, we see tarzan in his jungle environment. not being an expert, i am unaware as to the realism of its depiction of ape community life, but it is certainly entertaining. for me, the more moving section is the second half, when tarzan must meet his real family, develop language skills, and adjust to aristocratic british society, all the while wooing jane (andie macdowell). he is portrayed as a &#x27;noble savage&#x27;, whether in the wild or in elegant edwardian parlors. by contrast, the upper crust is depicted as often far more barbaric than the jungle tarzan left. christopher lambert is fantastic in his sympathetic portrayal of tarzan in both the jungle and civilized environments. he conveys a real sense of his confusion and conflict, torn as he is between the two very different worlds, his original ape family and his new human one. sir ralph richardson, one of the old british legends, is brilliant as always in the role of tarzan&#x27;s grandfather, the sixth earl of greystoke. the film focuses more on tarzan&#x27;s struggles in adapting to civilization and his inner conflict than on his jungle exploits. this unusual take on the old classic makes it both the typical dramatic adventure but also, above all, a moving personal story. i wasn&#x27;t surprised to note here that its director is the same individual, hugh hudson, who also directed chariots of fire, another brilliant movie.</span>\n",
              "    </p>\n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "    \n",
              "\n",
              "\n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "print('Positive' if test_df['is_positive'].iloc[1] else 'Negative')\n",
        "eli5.show_prediction(classifier, test_df['review'].iloc[1], vec=vectorizer,\n",
        "                     targets=['positive'], target_names=['negative', 'positive'], top=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fZ6I8mN0VPU"
      },
      "source": [
        "## Подключаем лингвистику"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkywIvbp4N-L"
      },
      "source": [
        "### Лемматизация и стемминг\n",
        "\n",
        "Если присмотреться, можно найти формы одного слова с разной семантической окраской по мнению классификатора. Или нет?\n",
        "\n",
        "**Задание** Найти формы слова с разной семантической окраской.\n",
        "\n",
        "Поверя, что они есть, попробуем что-нибудь с этим сделать.\n",
        "\n",
        "Например, лемматизируем - сведем к начальной форме все слова. Поможет в этом библиотека spacy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "S5CTTdtxI5yv"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "Ztne5BHOuIgA"
      },
      "outputs": [],
      "source": [
        "docs = [doc for doc in nlp.pipe(train_df.review.values[:50])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bXs8Ia_bqeS0",
        "outputId": "e5a21c7e-bc12-4774-c108-6b1e9faca243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dreamgirls dreamgirl B ORG\n",
            ", , O \n",
            "despite despite O \n",
            "its its O \n",
            "fistful fistful O \n",
            "of of O \n",
            "Tony Tony B PERSON\n",
            "wins win O \n",
            "in in O \n",
            "an an B DATE\n",
            "incredibly incredibly I DATE\n",
            "weak weak I DATE\n",
            "year year I DATE\n",
            "on on O \n",
            "Broadway Broadway B FAC\n",
            ", , O \n",
            "has have O \n",
            "never never O \n",
            "been be O \n",
            "what what O \n",
            "one one O \n",
            "would would O \n",
            "call call O \n",
            "a a O \n",
            "jewel jewel O \n",
            "in in O \n",
            "the the O \n",
            "crown crown O \n",
            "of of O \n",
            "stage stage O \n",
            "musicals musical O \n",
            ". . O \n",
            "However however O \n",
            ", , O \n",
            "that that O \n",
            "is be O \n",
            "not not O \n",
            "to to O \n",
            "say say O \n",
            "that that O \n",
            "in in O \n",
            "the the O \n",
            "right right O \n",
            "cinematic cinematic O \n",
            "hands hand O \n",
            "it it O \n",
            "could could O \n",
            "not not O \n",
            "be be O \n",
            "fleshed flesh O \n",
            "out out O \n",
            "and and O \n",
            "polished polish O \n",
            "into into O \n",
            "something something O \n",
            "worthwhile worthwhile O \n",
            "on on O \n",
            "- - O \n",
            "screen screen O \n",
            ". . O \n",
            "Unfortunately unfortunately O \n",
            ", , O \n",
            "what what O \n",
            "transfers transfer O \n",
            "to to O \n",
            "the the O \n",
            "screen screen O \n",
            "is be O \n",
            "basically basically O \n",
            "a a O \n",
            "slavishly slavishly O \n",
            "faithful faithful O \n",
            "version version O \n",
            "of of O \n",
            "the the O \n",
            "stage stage O \n",
            "hit hit O \n",
            "with with O \n",
            "all all O \n",
            "of of O \n",
            "its its O \n",
            "inherent inherent O \n",
            "weaknesses weakness O \n",
            "intact intact O \n",
            ". . O \n",
            "First first B ORDINAL\n",
            ", , O \n",
            "the the O \n",
            "score score O \n",
            "has have O \n",
            "never never O \n",
            "been be O \n",
            "one one O \n",
            "of of O \n",
            "the the O \n",
            "strong strong O \n",
            "points point O \n",
            "of of O \n",
            "this this O \n",
            "production production O \n",
            "and and O \n",
            "the the O \n",
            "film film O \n",
            "does do O \n",
            "not not O \n",
            "change change O \n",
            "that that O \n",
            "factor factor O \n",
            ". . O \n",
            "There there O \n",
            "are be O \n",
            "lots lot O \n",
            "of of O \n",
            "songs song O \n",
            "( ( O \n",
            "perhaps perhaps O \n",
            "too too O \n",
            "many many O \n",
            "? ? O \n",
            ") ) O \n",
            ", , O \n",
            "but but O \n",
            "few few O \n",
            "of of O \n",
            "them they O \n",
            "are be O \n",
            "especially especially O \n",
            "memorable memorable O \n",
            ". . O \n",
            "The the O \n",
            "closest close O \n",
            "any any O \n",
            "come come O \n",
            "to to O \n",
            "catchy catchy O \n",
            "tunes tune O \n",
            "are be O \n",
            "the the O \n",
            "title title O \n",
            "song song O \n",
            "and and O \n",
            "One one B CARDINAL\n",
            "Night night O \n",
            "Only only O \n",
            "- - O \n",
            "the the O \n",
            "much much O \n",
            "acclaimed acclaimed O \n",
            "And and O \n",
            "I I O \n",
            "Am be O \n",
            "Telling tell O \n",
            "You you O \n",
            "That that O \n",
            "I I O \n",
            "Am be O \n",
            "Not not O \n",
            "Going go O \n",
            "is be O \n",
            "less less O \n",
            "a a O \n",
            "great great O \n",
            "song song O \n",
            "than than O \n",
            "it it O \n",
            "is be O \n",
            "a a O \n",
            "dramatic dramatic O \n",
            "set set O \n",
            "piece piece O \n",
            "for for O \n",
            "the the O \n",
            "character character O \n",
            "of of O \n",
            "Effie Effie B ORG\n",
            "( ( O \n",
            "Jennifer Jennifer B PERSON\n",
            "Hudson Hudson I PERSON\n",
            ") ) O \n",
            ". . O \n",
            "The the O \n",
            "film film O \n",
            "is be O \n",
            "slick slick O \n",
            "and and O \n",
            "technically technically O \n",
            "well well O \n",
            "- - O \n",
            "produced produce O \n",
            ", , O \n",
            "but but O \n",
            "the the O \n",
            "story story O \n",
            "and and O \n",
            "characters character O \n",
            "are be O \n",
            "surprisingly surprisingly O \n",
            "thin thin O \n",
            "and and O \n",
            "lacking lack O \n",
            "in in O \n",
            "any any O \n",
            "resonance resonance O \n",
            ". . O \n",
            "There there O \n",
            "is be O \n",
            "some some O \n",
            "interest interest O \n",
            "in in O \n",
            "the the O \n",
            "opening opening O \n",
            "moments moment O \n",
            ", , O \n",
            "watching watch O \n",
            "Jamie Jamie B PERSON\n",
            "Foxx Foxx I PERSON\n",
            "'s 's I PERSON\n",
            "Svengali Svengali O \n",
            "- - O \n",
            "like like O \n",
            "manager manager O \n",
            "manipulate manipulate O \n",
            "his his O \n",
            "acts act O \n",
            "to to O \n",
            "the the O \n",
            "top top O \n",
            ", , O \n",
            "but but O \n",
            "that that O \n",
            "takes take O \n",
            "a a O \n",
            "back back O \n",
            "seat seat O \n",
            "in in O \n",
            "the the O \n",
            "latter latter O \n",
            "portion portion O \n",
            "of of O \n",
            "the the O \n",
            "film film O \n",
            ", , O \n",
            "when when O \n",
            "the the O \n",
            "story story O \n",
            "conveniently conveniently O \n",
            "tries try O \n",
            "to to O \n",
            "cast cast O \n",
            "him he O \n",
            "as as O \n",
            "a a O \n",
            "villain villain O \n",
            ", , O \n",
            "despite despite O \n",
            "his his O \n",
            "having have O \n",
            "been be O \n",
            "right right O \n",
            "from from O \n",
            "a a O \n",
            "business business O \n",
            "stand stand O \n",
            "- - O \n",
            "point point O \n",
            "for for O \n",
            "a a O \n",
            "good good O \n",
            "majority majority O \n",
            "of of O \n",
            "the the O \n",
            "film film O \n",
            ". . O \n",
            "Beyonce Beyonce B PERSON\n",
            "Knowles Knowles I PERSON\n",
            "is be O \n",
            "lovely lovely O \n",
            "and and O \n",
            "sings sing O \n",
            "her her O \n",
            "songs song O \n",
            "perfectly perfectly O \n",
            "well well O \n",
            ", , O \n",
            "but but O \n",
            "is be O \n",
            "stuck stick O \n",
            "with with O \n",
            "a a O \n",
            "character character O \n",
            "who who O \n",
            "is be O \n",
            "basically basically O \n",
            "all all O \n",
            "surface surface O \n",
            "glitz glitz O \n",
            ". . O \n",
            "Anika Anika B PERSON\n",
            "Noni Noni I PERSON\n",
            "Rose Rose I PERSON\n",
            "as as O \n",
            "the the O \n",
            "third third B ORDINAL\n",
            "member member O \n",
            "of of O \n",
            "the the O \n",
            "Dreamgirls Dreamgirls B ORG\n",
            "trio trio O \n",
            "literally literally O \n",
            "has have O \n",
            "nothing nothing O \n",
            "to to O \n",
            "do do O \n",
            "for for O \n",
            "the the O \n",
            "entire entire O \n",
            "film film O \n",
            ". . O \n",
            "Eddie Eddie B PERSON\n",
            "Murphy Murphy I PERSON\n",
            "acquits acquit O \n",
            "himself himself O \n",
            "well well O \n",
            "as as O \n",
            "a a O \n",
            "singer singer O \n",
            "obviously obviously O \n",
            "based base O \n",
            "on on O \n",
            "James James B PERSON\n",
            "Brown Brown I PERSON\n",
            ", , O \n",
            "but but O \n",
            "the the O \n",
            "role role O \n",
            "is be O \n",
            "not not O \n",
            "especially especially O \n",
            "meaty meaty O \n",
            "and and O \n",
            "ultimately ultimately O \n",
            "has have O \n",
            "little little O \n",
            "impact impact O \n",
            ". . O \n",
            "Foxx Foxx O \n",
            "would would O \n",
            "seem seem O \n",
            "ideal ideal O \n",
            "casting casting O \n",
            ", , O \n",
            "but but O \n",
            "he he O \n",
            "seems seem O \n",
            "oddly oddly O \n",
            "withdrawn withdraw O \n",
            "and and O \n",
            "bored bored O \n",
            ". . O \n",
            "The the O \n",
            "film film O \n",
            "'s 's O \n",
            "biggest big O \n",
            "selling selling O \n",
            "point point O \n",
            "is be O \n",
            "surely surely O \n",
            "former former O \n",
            "American American B ORG\n",
            "Idol Idol I ORG\n",
            "contestant contestant O \n",
            "/ / O \n",
            "Oscar Oscar O \n",
            "winner winner O \n",
            "Jennifer Jennifer B PERSON\n",
            "Hudson Hudson I PERSON\n",
            "in in O \n",
            "the the O \n",
            "central central O \n",
            "role role O \n",
            "of of O \n",
            "Effie Effie B ORG\n",
            "White White O \n",
            ", , O \n",
            "the the O \n",
            "temperamental temperamental O \n",
            "singer singer O \n",
            "who who O \n",
            "gets get O \n",
            "booted boot O \n",
            "from from O \n",
            "the the O \n",
            "group group O \n",
            "and and O \n",
            "makes make O \n",
            "a a O \n",
            "triumphant triumphant O \n",
            "closing closing O \n",
            "act act O \n",
            "return return O \n",
            ". . O \n",
            "For for O \n",
            "me I O \n",
            ", , O \n",
            "Effie Effie B ORG\n",
            "has have O \n",
            "always always O \n",
            "been be O \n",
            "a a O \n",
            "big big O \n",
            "problem problem O \n",
            "in in O \n",
            "both both O \n",
            "the the O \n",
            "show show O \n",
            "and and O \n",
            "the the O \n",
            "movie movie O \n",
            ". . O \n",
            "The the O \n",
            "film film O \n",
            "obviously obviously O \n",
            "wants want O \n",
            "you you O \n",
            "to to O \n",
            "feel feel O \n",
            "sorry sorry O \n",
            "for for O \n",
            "her she O \n",
            "and and O \n",
            "rather rather O \n",
            "ham ham O \n",
            "- - O \n",
            "handedly handedly O \n",
            "takes take O \n",
            "her her O \n",
            "side side O \n",
            ", , O \n",
            "but but O \n",
            "I I O \n",
            "have have O \n",
            "never never O \n",
            "been be O \n",
            "sure sure O \n",
            "that that O \n",
            "this this O \n",
            "character character O \n",
            "deserves deserve O \n",
            "that that O \n",
            "kind kind O \n",
            "of of O \n",
            "devotion devotion O \n",
            ". . O \n",
            "From from O \n",
            "the the O \n",
            "start start O \n",
            ", , O \n",
            "Effie Effie B ORG\n",
            "conducts conduct O \n",
            "herself herself O \n",
            "for for O \n",
            "the the O \n",
            "most most O \n",
            "part part O \n",
            "like like O \n",
            "an an O \n",
            "obnoxious obnoxious O \n",
            ", , O \n",
            "egotistical egotistical O \n",
            ", , O \n",
            "self self O \n",
            "- - O \n",
            "centered center O \n",
            "diva diva O \n",
            ", , O \n",
            "who who O \n",
            "is be O \n",
            "more more O \n",
            "interested interested O \n",
            "in in O \n",
            "what what O \n",
            "everyone everyone O \n",
            "else else O \n",
            "can can O \n",
            "do do O \n",
            "for for O \n",
            "her she O \n",
            "rather rather O \n",
            "than than O \n",
            "having have O \n",
            "much much O \n",
            "vested vested O \n",
            "interest interest O \n",
            "in in O \n",
            "the the O \n",
            "group group O \n",
            "of of O \n",
            "which which O \n",
            "she she O \n",
            "is be O \n",
            "a a O \n",
            "part part O \n",
            ". . O \n",
            "When when O \n",
            "she she O \n",
            "is be O \n",
            "booted boot O \n",
            "from from O \n",
            "the the O \n",
            "group group O \n",
            "for for O \n",
            "her her O \n",
            "unprofessionalism unprofessionalism O \n",
            "and and O \n",
            "bad bad O \n",
            "attitude attitude O \n",
            ", , O \n",
            "the the O \n",
            "charges charge O \n",
            "are be O \n",
            "more more O \n",
            "than than O \n",
            "well well O \n",
            "- - O \n",
            "founded found O \n",
            ", , O \n",
            "but but O \n",
            "the the O \n",
            "stage stage O \n",
            "show show O \n",
            "/ / O \n",
            "film film O \n",
            "seem seem O \n",
            "to to O \n",
            "think think O \n",
            "Effie Effie B ORG\n",
            "should should O \n",
            "be be O \n",
            "cut cut O \n",
            "unlimited unlimited O \n",
            "slack slack O \n",
            "simply simply O \n",
            "because because O \n",
            "she she O \n",
            "has have O \n",
            "a a O \n",
            "great great O \n",
            "voice voice O \n",
            ". . O \n",
            "Even even O \n",
            "though though O \n",
            "the the O \n",
            "film film O \n",
            "tries try O \n",
            "to to O \n",
            "soften soften O \n",
            "some some O \n",
            "of of O \n",
            "Effie Effie B ORG\n",
            "'s 's O \n",
            "harder hard O \n",
            "edges edge O \n",
            "to to O \n",
            "make make O \n",
            "her she O \n",
            "more more O \n",
            "likable likable O \n",
            ", , O \n",
            "the the O \n",
            "charges charge O \n",
            "still still O \n",
            "stand stand O \n",
            ". . O \n",
            "Her her O \n",
            "story story O \n",
            "becomes become O \n",
            "more more O \n",
            "manipulative manipulative O \n",
            "by by O \n",
            "suggesting suggest O \n",
            "she she O \n",
            "should should O \n",
            "have have O \n",
            "our our O \n",
            "further further O \n",
            "sympathy sympathy O \n",
            "because because O \n",
            "she she O \n",
            "is be O \n",
            "an an O \n",
            "unwed unwed O \n",
            "mother mother O \n",
            "struggling struggle O \n",
            "to to O \n",
            "raise raise O \n",
            "her her O \n",
            "daughter daughter O \n",
            "- - O \n",
            "using use O \n",
            "the the O \n",
            "implication implication O \n",
            "that that O \n",
            "( ( O \n",
            "much much O \n",
            "like like O \n",
            "the the O \n",
            "talent talent O \n",
            "card card O \n",
            ") ) O \n",
            "motherhood motherhood O \n",
            "immediately immediately O \n",
            "makes make O \n",
            "any any O \n",
            "behavior behavior O \n",
            "excusable excusable O \n",
            ". . O \n",
            "Indeed indeed O \n",
            "the the O \n",
            "only only O \n",
            "big big O \n",
            "effort effort O \n",
            "the the O \n",
            "film film O \n",
            "makes make O \n",
            "to to O \n",
            "show show O \n",
            "Effie Effie B ORG\n",
            "'s 's O \n",
            "mothering mothering O \n",
            "is be O \n",
            "to to O \n",
            "tell tell O \n",
            "us we O \n",
            "about about O \n",
            "it it O \n",
            "and and O \n",
            "then then O \n",
            "include include O \n",
            "a a O \n",
            "scene scene O \n",
            "where where O \n",
            "she she O \n",
            "barks bark O \n",
            "at at O \n",
            "her her O \n",
            "daughter daughter O \n",
            "in in O \n",
            "the the O \n",
            "unemployment unemployment O \n",
            "office office O \n",
            ", , O \n",
            "insists insist O \n",
            "that that O \n",
            "the the O \n",
            "girl girl O \n",
            "has have O \n",
            "\" \" O \n",
            "no no O \n",
            "father father O \n",
            "\" \" O \n",
            "and and O \n",
            "then then O \n",
            "refuse refuse O \n",
            "to to O \n",
            "look look O \n",
            "for for O \n",
            "gainful gainful O \n",
            "employment employment O \n",
            "to to O \n",
            "support support O \n",
            "them they O \n",
            "since since O \n",
            "singing singing O \n",
            "is be O \n",
            "all all O \n",
            "she she O \n",
            "knows know O \n",
            ". . O \n",
            "In in O \n",
            "the the O \n",
            "hands hand O \n",
            "of of O \n",
            "a a O \n",
            "skillful skillful O \n",
            "actress actress O \n",
            ", , O \n",
            "the the O \n",
            "gaps gap O \n",
            "could could O \n",
            "perhaps perhaps O \n",
            "have have O \n",
            "been be O \n",
            "remedied remedied O \n",
            "with with O \n",
            "technique technique O \n",
            "and and O \n",
            "charisma charisma O \n",
            ". . O \n",
            "Unfortunately unfortunately O \n",
            ", , O \n",
            "Hudson Hudson B PERSON\n",
            "is be O \n",
            "not not O \n",
            "that that O \n",
            "actress actress O \n",
            ". . O \n",
            "She she O \n",
            "sings sing O \n",
            "well well O \n",
            ", , O \n",
            "but but O \n",
            "the the O \n",
            "dialog dialog O \n",
            "- - O \n",
            "driven drive O \n",
            "moments moment O \n",
            "do do O \n",
            "not not O \n",
            "come come O \n",
            "naturally naturally O \n",
            "to to O \n",
            "her she O \n",
            "nor nor O \n",
            "do do O \n",
            "high high O \n",
            "emotional emotional O \n",
            "moments moment O \n",
            ". . O \n",
            "Effie Effie B ORG\n",
            "'s 's O \n",
            "signature signature O \n",
            "moment moment O \n",
            "( ( O \n",
            "the the O \n",
            "aforementioned aforementione O \n",
            "And and O \n",
            "I I O \n",
            "Am be O \n",
            "Telling tell O \n",
            "You you O \n",
            "... ... O \n",
            "number number O \n",
            ") ) O \n",
            "is be O \n",
            "well well O \n",
            "- - O \n",
            "sung sing O \n",
            "by by O \n",
            "Hudson Hudson B PERSON\n",
            ", , O \n",
            "but but O \n",
            "emotionally emotionally O \n",
            "flat flat O \n",
            "in in O \n",
            "the the O \n",
            "acting act O \n",
            "department department O \n",
            ". . O \n",
            "Effie Effie B ORG\n",
            "is be O \n",
            "supposed suppose O \n",
            "to to O \n",
            "expressing express O \n",
            "her her O \n",
            "rage rage O \n",
            "and and O \n",
            "desperation desperation O \n",
            "at at O \n",
            "her her O \n",
            "predicament predicament O \n",
            ", , O \n",
            "but but O \n",
            "Hudson Hudson B PERSON\n",
            "comes come O \n",
            "off off O \n",
            "as as O \n",
            "a a O \n",
            "cabaret cabaret O \n",
            "performer performer O \n",
            "belting belt O \n",
            "out out O \n",
            "a a O \n",
            "hot hot O \n",
            "number number O \n",
            ". . O \n",
            "All all O \n",
            "in in O \n",
            "all all O \n",
            ", , O \n",
            "not not O \n",
            "quite quite O \n",
            "the the O \n",
            "emotional emotional O \n",
            "highlight highlight O \n",
            "one one O \n",
            "expects expect O \n",
            ". . O \n",
            "The the O \n",
            "latter latter O \n",
            "portion portion O \n",
            "of of O \n",
            "the the O \n",
            "film film O \n",
            "is be O \n",
            "basically basically O \n",
            "a a O \n",
            "predictable predictable O \n",
            "melange melange O \n",
            "of of O \n",
            "events event O \n",
            "that that O \n",
            "maneuver maneuver O \n",
            "Foxx Foxx B ORG\n",
            "into into O \n",
            "Hudson Hudson B ORG\n",
            "'s 's O \n",
            "earlier early O \n",
            "position position O \n",
            "and and O \n",
            "allow allow O \n",
            "her she O \n",
            "to to O \n",
            "strut strut O \n",
            "back back O \n",
            "in in O \n",
            "and and O \n",
            "lord lord O \n",
            "it it O \n",
            "over over O \n",
            "everyone everyone O \n",
            ". . O \n",
            "Foxx Foxx B ORG\n",
            "'s 's O \n",
            "criminal criminal O \n",
            "offenses offense O \n",
            "in in O \n",
            "the the O \n",
            "film film O \n",
            "are be O \n",
            "undoubtedly undoubtedly O \n",
            "par par O \n",
            "for for O \n",
            "the the O \n",
            "course course O \n",
            "of of O \n",
            "many many O \n",
            "struggling struggle O \n",
            "record record O \n",
            "producers producer O \n",
            ", , O \n",
            "but but O \n",
            "the the O \n",
            "film film O \n",
            "'s 's O \n",
            "seeming seeming O \n",
            "implication implication O \n",
            "that that O \n",
            "he he O \n",
            "has have O \n",
            "it it O \n",
            "coming come O \n",
            "because because O \n",
            "he he O \n",
            "helped help O \n",
            "usher usher O \n",
            "in in O \n",
            "the the O \n",
            "disco disco O \n",
            "era era O \n",
            "is be O \n",
            "rather rather O \n",
            "ridiculous ridiculous O \n",
            ", , O \n",
            "not not O \n",
            "to to O \n",
            "mention mention O \n",
            "pretentious pretentious O \n",
            "and and O \n",
            "condescending condescend O \n",
            ", , O \n",
            "particularly particularly O \n",
            "coming come O \n",
            "from from O \n",
            "a a O \n",
            "film film O \n",
            "with with O \n",
            "all all O \n",
            "of of O \n",
            "the the O \n",
            "depth depth O \n",
            "of of O \n",
            "a a O \n",
            "puddle puddle O \n",
            ". . O \n",
            "The the O \n",
            "end end O \n",
            "result result O \n",
            "is be O \n",
            "a a O \n",
            "faithful faithful O \n",
            "rendition rendition O \n",
            "of of O \n",
            "the the O \n",
            "stage stage O \n",
            "hit hit O \n",
            ", , O \n",
            "drained drain O \n",
            "of of O \n",
            "emotion emotion O \n",
            ", , O \n",
            "energy energy O \n",
            "or or O \n",
            "anything anything O \n",
            "that that O \n",
            "can can O \n",
            "be be O \n",
            "described describe O \n",
            "as as O \n",
            "dynamic dynamic O \n",
            ". . O \n"
          ]
        }
      ],
      "source": [
        "for token in docs[0]:\n",
        "    print(token.text, token.lemma_, token.ent_iob_, token.ent_type_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVXJr_xxqlPK"
      },
      "source": [
        "Весь этот процесс очень долгий, поэтому я предподсчитал всё."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "3ZNz7E5JqrWz"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('train_docs.pkl', 'rb') as f:\n",
        "    train_docs = pickle.load(f)\n",
        "\n",
        "with open('test_docs.pkl', 'rb') as f:\n",
        "    test_docs = pickle.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAXHi9tC6L4p"
      },
      "source": [
        "# task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "nVgo50d1xB3Q"
      },
      "outputs": [],
      "source": [
        " def lemmatize_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens=[token.lemma_.strip() for token in doc\n",
        "            #  if (not token.is_stop) or token.lemma_ in ['no','not']\n",
        "            ]\n",
        "    text=\" \".join(tokens)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "C1wm8J5sx1Dp"
      },
      "outputs": [],
      "source": [
        "train_lem_df = train_df\n",
        "test_lem_df = test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "IkLZOlznx4_3"
      },
      "outputs": [],
      "source": [
        "train_lem_df['review'] = train_lem_df['review'].apply(lambda x: lemmatize_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eFIBBm_9x7EF"
      },
      "outputs": [],
      "source": [
        "test_lem_df['review'] = test_lem_df['review'].apply(lambda x: lemmatize_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "FcUfD4FbyLGv",
        "outputId": "c90e7e3b-4193-48ab-b8f7-f60952be6339"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'dreamgirl , despite its fistful of Tony win in an incredibly weak year on Broadway , have never be what one would call a jewel in the crown of stage musical . however , that be not to say that in the right cinematic hand it could not be flesh out and polish into something worthwhile on - screen . unfortunately , what transfer to the screen be basically a slavishly faithful version of the stage hit with all of its inherent weakness intact . first , the score have never be one of the strong point of this production and the film do not change that factor . there be lot of song ( perhaps too many ? ) , but few of they be especially memorable . the close any come to catchy tune be the title song and one night only - the much acclaimed and I be tell you that I be not go be less a great song than it be a dramatic set piece for the character of Effie ( Jennifer Hudson ) . the film be slick and technically well - produce , but the story and character be surprisingly thin and lack in any resonance . there be some interest in the opening moment , watch Jamie Foxx \\'s Svengali - like manager manipulate his act to the top , but that take a back seat in the latter portion of the film , when the story conveniently try to cast he as a villain , despite his have be right from a business stand - point for a good majority of the film . Beyonce Knowles be lovely and sing her song perfectly well , but be stick with a character who be basically all surface glitz . Anika Noni Rose as the third member of the Dreamgirls trio literally have nothing to do for the entire film . Eddie Murphy acquit himself well as a singer obviously base on James Brown , but the role be not especially meaty and ultimately have little impact . Foxx would seem ideal casting , but he seem oddly withdraw and bored . the film \\'s big selling point be surely former American Idol contestant / Oscar winner Jennifer Hudson in the central role of Effie White , the temperamental singer who get boot from the group and make a triumphant closing act return . for I , Effie have always be a big problem in both the show and the movie . the film obviously want you to feel sorry for she and rather ham - handedly take her side , but I have never be sure that this character deserve that kind of devotion . from the start , Effie conduct herself for the most part like an obnoxious , egotistical , self - center diva , who be more interested in what everyone else can do for she rather than have much vested interest in the group of which she be a part . when she be boot from the group for her unprofessionalism and bad attitude , the charge be more than well - found , but the stage show / film seem to think Effie should be cut unlimited slack simply because she have a great voice . even though the film try to soften some of Effie \\'s hard edge to make she more likable , the charge still stand . her story become more manipulative by suggest she should have our further sympathy because she be an unwed mother struggle to raise her daughter - use the implication that ( much like the talent card ) motherhood immediately make any behavior excusable . indeed the only big effort the film make to show Effie \\'s mothering be to tell we about it and then include a scene where she bark at her daughter in the unemployment office , insist that the girl have \" no father \" and then refuse to look for gainful employment to support they since singing be all she know . in the hand of a skillful actress , the gap could perhaps have be remedied with technique and charisma . unfortunately , Hudson be not that actress . she sing well , but the dialog - drive moment do not come naturally to she nor do high emotional moment . Effie \\'s signature moment ( the aforementione and I be tell you ... number ) be well - sing by Hudson , but emotionally flat in the act department . Effie be suppose to express her rage and desperation at her predicament , but Hudson come off as a cabaret performer belt out a hot number . all in all , not quite the emotional highlight one expect . the latter portion of the film be basically a predictable melange of event that maneuver Foxx into Hudson \\'s early position and allow she to strut back in and lord it over everyone . Foxx \\'s criminal offense in the film be undoubtedly par for the course of many struggle record producer , but the film \\'s seeming implication that he have it come because he help usher in the disco era be rather ridiculous , not to mention pretentious and condescend , particularly come from a film with all of the depth of a puddle . the end result be a faithful rendition of the stage hit , drain of emotion , energy or anything that can be describe as dynamic .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "train_lem_df['review'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llOrNpcbxBCk",
        "outputId": "bc4ee0c1-a147-40fa-8301-69b2fb9147a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 87.95%\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(train_lem_df['review'], train_lem_df['is_positive'])\n",
        "\n",
        "eval_model(model, test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выводы\n",
        "Лематизация улучшила метрики за счет уменьшения размера вектра если бы еще провести очистку тектса то модель бы работала еще лучше"
      ],
      "metadata": {
        "id": "AXhC0zEisr8t"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u30e5sEz6QP4"
      },
      "source": [
        "# task 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "lvlZjCOMzVEI"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0QJpIr0WzmZH"
      },
      "outputs": [],
      "source": [
        "def tagging_text(text):\n",
        "    doc = nlp(text)\n",
        "    tokens=[token.ent_type_.strip() if token.ent_type_ !=\"\" else token.text.strip() for token in doc ]\n",
        "\n",
        "    # Удалим повотрояющиеся токены.\n",
        "    text = [tokens[i] for i in range(1, len(tokens)) if tokens[i] != tokens[i-1] ]\n",
        "    text=\" \".join(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "9oW-vr5jzoe0"
      },
      "outputs": [],
      "source": [
        "train_tag_df = train_lem_df\n",
        "test_tag_df = test_lem_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "Zf4Frlpzzqq2"
      },
      "outputs": [],
      "source": [
        "train_tag_df['review'] = train_tag_df['review'].apply(lambda x: tagging_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Bw2F1BwhzsOn"
      },
      "outputs": [],
      "source": [
        "test_tag_df['review'] = test_tag_df['review'].apply(lambda x: tagging_text(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "mN4YOKguztvn",
        "outputId": "8458209b-5008-4283-a4c3-c774507bd0bf"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "', despite its fistful of PERSON win in DATE on FAC , have never be what one would call a jewel in the crown of stage musical . however , that be not to say that in the right cinematic hand it could not be flesh out and NORP into something worthwhile on - screen . unfortunately , what transfer to the screen be basically a slavishly faithful version of the stage hit with all of its inherent weakness intact . ORDINAL , the score have never be one of the strong point of this production and the film do not change that factor . there be lot of song ( perhaps too many ? ) , but few of they be especially memorable . the close any come to catchy tune be the title song and TIME only - the much acclaimed and I be tell you that I be not go be less a great song than it be a dramatic set piece for the character of ORG ( PERSON ) . the film be slick and technically well - produce , but the story and character be surprisingly thin and lack in any resonance . there be some interest in the opening moment , watch PERSON Svengali - like manager manipulate his act to the top , but that take a back seat in the latter portion of the film , when the story conveniently try to cast he as a villain , despite his have be right from a business stand - point for a good majority of the film . PERSON be lovely and sing her song perfectly well , but be stick with a character who be basically all surface glitz . PERSON as the ORDINAL member of the ORG trio literally have nothing to do for the entire film . PERSON acquit himself well as a singer obviously base on PERSON , but the role be not especially meaty and ultimately have little impact . Foxx would seem ideal casting , but he seem oddly withdraw and bored . the film \\'s big selling point be surely former ORG contestant / Oscar winner PERSON in the central role of ORG White , the temperamental singer who get boot from the group and make a triumphant closing act return . for I , ORG have always be a big problem in both the show and the movie . the film obviously want you to feel sorry for she and rather ham - handedly take her side , but I have never be sure that this character deserve that kind of devotion . from the start , ORG conduct herself for the most part like an obnoxious , egotistical , self - center diva , who be more interested in what everyone else can do for she rather than have much vested interest in the group of which she be a part . when she be boot from the group for her unprofessionalism and bad attitude , the charge be more than well - found , but the stage show / film seem to think ORG should be cut unlimited slack simply because she have a great voice . even though the film try to soften some of ORG \\'s hard edge to make she more likable , the charge still stand . her story become more manipulative by suggest she should have our further sympathy because she be an unwed mother struggle to raise her daughter - use the implication that ( much like the talent card ) motherhood immediately make any behavior excusable . indeed the only big effort the film make to show ORG \\'s mothering be to tell we about it and then include a scene where she bark at her daughter in the unemployment office , insist that the girl have \" no father \" and then refuse to look for gainful employment to support they since singing be all she know . in the hand of a skillful actress , the gap could perhaps have be remedied with technique and charisma . unfortunately , PERSON be not that actress . she sing well , but the dialog - drive moment do not come naturally to she nor do high emotional moment . ORG \\'s signature moment ( the aforementione and I be tell you ... number ) be well - sing by PERSON , but emotionally flat in the act department . ORG be suppose to express her rage and desperation at her predicament , but PERSON come off as a cabaret performer belt out a hot number . all in all , not quite the emotional highlight one expect . the latter portion of the film be basically a predictable melange of event that maneuver ORG into PERSON \\'s early position and allow she to strut back in and lord it over everyone . ORG \\'s criminal offense in the film be undoubtedly par for the course of many struggle record producer , but the film \\'s seeming implication that he have it come because he help usher in the disco era be rather ridiculous , not to mention pretentious and condescend , particularly come from a film with all of the depth of a puddle . the end result be a faithful rendition of the stage hit , drain of emotion , energy or anything that can be describe as dynamic .'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "train_tag_df['review'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PUtX8DxbzyJp",
        "outputId": "57b1f487-b587-4689-a10b-1d7c0b9b87a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy = 87.78%\n"
          ]
        }
      ],
      "source": [
        "vectorizer = TfidfVectorizer(ngram_range=(2, 6), max_features=20000, analyzer='char')\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "model = Pipeline([\n",
        "    ('vectorizer', vectorizer),\n",
        "    ('classifier', classifier)\n",
        "])\n",
        "\n",
        "model.fit(train_tag_df['review'], train_tag_df['is_positive'])\n",
        "\n",
        "eval_model(model, test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "58WWJq0y3YDm"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Dropout, Conv1D, BatchNormalization, MaxPooling1D#, GlobalAveragePooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjRVTEHt3bif",
        "outputId": "4c6fdbdc-9acf-4917-fac2-2109b670d016"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count 12093\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "words_counter = Counter((word for text in train_tag_df.review for word in text.lower().split()))\n",
        "\n",
        "word2idx = {\n",
        "    '': 0,\n",
        "    '<unk>': 1\n",
        "}\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < 10:\n",
        "        break\n",
        "\n",
        "    word2idx[word] = len(word2idx)\n",
        "\n",
        "print('Words count', len(word2idx))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "H0D0nEq73dnz"
      },
      "outputs": [],
      "source": [
        "def convert(texts, word2idx, max_text_len):\n",
        "    data = np.zeros((len(texts), max_text_len), dtype=np.int64)\n",
        "\n",
        "    for inx, text in enumerate(texts):\n",
        "        result = []\n",
        "        for word in text.split():\n",
        "            if word in word2idx:\n",
        "                result.append(word2idx[word])\n",
        "        padding = [0]*(max_text_len - len(result))\n",
        "        data[inx] = np.array(padding + result[-max_text_len:], dtype=np.int64)\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "hLCqcwJT3fkk"
      },
      "outputs": [],
      "source": [
        "X_train = convert(train_tag_df.review, word2idx, 1000)\n",
        "X_test = convert(test_tag_df.review, word2idx, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "0nmDKNnQ3hZm"
      },
      "outputs": [],
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word2idx), output_dim=256, input_shape=(X_train.shape[1],)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(units=256, activation='relu'),\n",
        "    Dropout(0.32),\n",
        "    Dense(units=128, activation='relu'),\n",
        "    Dropout(0.32),\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCQ5l_8L3jxm",
        "outputId": "5dd99363-d8ab-4d6f-dd86-3f276e6a29e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "25/25 [==============================] - 10s 222ms/step - loss: 0.6879 - accuracy: 0.5454 - val_loss: 0.6654 - val_accuracy: 0.7616\n",
            "Epoch 2/5\n",
            "25/25 [==============================] - 5s 183ms/step - loss: 0.5433 - accuracy: 0.8062 - val_loss: 0.3533 - val_accuracy: 0.8506\n",
            "Epoch 3/5\n",
            "25/25 [==============================] - 4s 163ms/step - loss: 0.2879 - accuracy: 0.8804 - val_loss: 0.2914 - val_accuracy: 0.8781\n",
            "Epoch 4/5\n",
            "25/25 [==============================] - 6s 227ms/step - loss: 0.1925 - accuracy: 0.9288 - val_loss: 0.2838 - val_accuracy: 0.8836\n",
            "Epoch 5/5\n",
            "25/25 [==============================] - 4s 167ms/step - loss: 0.1234 - accuracy: 0.9592 - val_loss: 0.3041 - val_accuracy: 0.8820\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b42a6f82350>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ],
      "source": [
        "model.fit(X_train, train_tag_df.is_positive, batch_size=1024, epochs=5,\n",
        "          validation_data=(X_test, test_tag_df.is_positive))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_zQ4Phd3lwc",
        "outputId": "4af282e2-cc43-4151-dee9-c392d50ccceb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 0s 18ms/step - loss: 0.3041 - accuracy: 0.8820\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30409055948257446, 0.8820000290870667]"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "model.evaluate(X_test, test_tag_df.is_positive, batch_size=1024)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Pfup3O5r30m"
      },
      "source": [
        "**Задание** Сделайте классификатор на лемматизированных текстах.\n",
        "\n",
        "Более простой способ нормализации слов - использовать стемминг. Он немного тупой, не учитывает контекст, но иногда оказывается даже эффективнее лемматизации - а, главное, быстрее.\n",
        "\n",
        "По сути это просто набор правил, как обрезать слово, чтобы получить основу (stem):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cr0w_hVyrqFx",
        "outputId": "e659a81d-c001-4b9f-be20-090d4ba2bd5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "becom\n",
            "becom\n",
            "becam\n"
          ]
        }
      ],
      "source": [
        "from nltk import PorterStemmer\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "print(stemmer.stem('become'))\n",
        "print(stemmer.stem('becomes'))\n",
        "print(stemmer.stem('became'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxGSEqHNsfHy"
      },
      "source": [
        "**Задание** Попробуйте вместо лемм классифицировать основы."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1NBpkwuspBX"
      },
      "source": [
        "### NER\n",
        "\n",
        "В текстах рецензий очень много именованных сущностей. Вот, например:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 647
        },
        "id": "Xki8Maf9MrVY",
        "outputId": "b7260c66-6bc6-4fcc-c2e3-0738685197ef"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Dreamgirls\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              ", despite its fistful of \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Tony\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " wins in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    an incredibly weak year\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              " on \n",
              "<mark class=\"entity\" style=\"background: #9cc9cc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Broadway\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">FAC</span>\n",
              "</mark>\n",
              ", has never been what one would call a jewel in the crown of stage musicals. However, that is not to say that in the right cinematic hands it could not be fleshed out and polished into something worthwhile on-screen. Unfortunately, what transfers to the screen is basically a slavishly faithful version of the stage hit with all of its inherent weaknesses intact. \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    First\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              ", the score has never been one of the strong points of this production and the film does not change that factor. There are lots of songs (perhaps too many?), but few of them are especially memorable. The closest any come to catchy tunes are the title song and \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    One\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " Night Only - the much acclaimed And I Am Telling You That I Am Not Going is less a great song than it is a dramatic set piece for the character of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " (\n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jennifer Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              "). The film is slick and technically well-produced, but the story and characters are surprisingly thin and lacking in any resonance. There is some interest in the opening moments, watching \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jamie Foxx's\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " Svengali-like manager manipulate his acts to the top, but that takes a back seat in the latter portion of the film, when the story conveniently tries to cast him as a villain, despite his having been right from a business stand-point for a good majority of the film. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Beyonce Knowles\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is lovely and sings her songs perfectly well, but is stuck with a character who is basically all surface glitz. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Anika Noni Rose\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " as the \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    third\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
              "</mark>\n",
              " member of the \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Dreamgirls\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " trio literally has nothing to do for the entire film. \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Eddie Murphy\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " acquits himself well as a singer obviously based on \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    James Brown\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", but the role is not especially meaty and ultimately has little impact. Foxx would seem ideal casting, but he seems oddly withdrawn and bored. The film's biggest selling point is surely former \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American Idol\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " contestant/Oscar winner \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Jennifer Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " in the central role of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " White, the temperamental singer who gets booted from the group and makes a triumphant closing act return. For me, \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " has always been a big problem in both the show and the movie. The film obviously wants you to feel sorry for her and rather ham-handedly takes her side, but I have never been sure that this character deserves that kind of devotion. From the start, \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " conducts herself for the most part like an obnoxious, egotistical, self-centered diva, who is more interested in what everyone else can do for her rather than having much vested interest in the group of which she is a part. When she is booted from the group for her unprofessionalism and bad attitude, the charges are more than well-founded, but the stage show/film seem to think \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " should be cut unlimited slack simply because she has a great voice. Even though the film tries to soften some of \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s harder edges to make her more likable, the charges still stand. Her story becomes more manipulative by suggesting she should have our further sympathy because she is an unwed mother struggling to raise her daughter - using the implication that (much like the talent card) motherhood immediately makes any behavior excusable. Indeed the only big effort the film makes to show \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s mothering is to tell us about it and then include a scene where she barks at her daughter in the unemployment office, insists that the girl has &quot;no father&quot; and then refuse to look for gainful employment to support them since singing is all she knows. In the hands of a skillful actress, the gaps could perhaps have been remedied with technique and charisma. Unfortunately, \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " is not that actress. She sings well, but the dialog-driven moments do not come naturally to her nor do high emotional moments. \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s signature moment (the aforementioned And I Am Telling You... number) is well-sung by \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              ", but emotionally flat in the acting department. \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Effie\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " is supposed to expressing her rage and desperation at her predicament, but \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " comes off as a cabaret performer belting out a hot number. All in all, not quite the emotional highlight one expects. The latter portion of the film is basically a predictable melange of events that maneuver \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Foxx\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " into \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Hudson\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s earlier position and allow her to strut back in and lord it over everyone. \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Foxx\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              "'s criminal offenses in the film are undoubtedly par for the course of many struggling record producers, but the film's seeming implication that he has it coming because he helped usher in the disco era is rather ridiculous, not to mention pretentious and condescending, particularly coming from a film with all of the depth of a puddle. The end result is a faithful rendition of the stage hit, drained of emotion, energy or anything that can be described as dynamic.</div></span>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "displacy.render(docs[0], style='ent', jupyter=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGhcnuZSteHc"
      },
      "source": [
        "Вообще говоря, почему вдруг какой-нибудь Депп должен нести семантическую окраску? Однако оказывается, что классификатор выучивает, что какие-то имена чаще в положительных рецензиях - или наоборот. Это похоже на переобучение - почему бы не попробовать вырезать сущности?\n",
        "\n",
        "**Задание** Удалите из текстов какие-то из сущностей, пользуясь координатами из запикленных файлов. Описание сущностей можно посмотреть [здесь](https://spacy.io/api/annotation#named-entities). Запустите классификатор."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k2fHA70b0zEZ"
      },
      "source": [
        "## Включаем дип лёрнинг"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1VCrM671XO5"
      },
      "source": [
        "Мы тут пришли deep learning'ом заниматься, а делаем почему-то модель на логистической регрессии. Как так?\n",
        "\n",
        "Попробуем запустить относительно стандартную модель для классификации текстов - сверточная сеть поверх словных эмбеддингов.\n",
        "\n",
        "Разбираться, что это за зверь, будем на следующих занятиях, а пока будем просто им пользоваться :)\n",
        "\n",
        "Каждое предложение нужно представлять набором слов - и сразу же начинаются проблемы. Во-первых, как ограничить длину предложения?\n",
        "\n",
        "Прикинем по гистограмме, какая длина нам подходит:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02Kv8YUPbhzG",
        "outputId": "46f38cf9-8298-462a-d8e2-0433e1df4b1b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        , despite its fistful of PERSON win in DATE on...\n",
              "1        show come up with interesting location as fast...\n",
              "2        simply love this movie . I also love the ORG ,...\n",
              "3        ahead if you want to call they that ...  I wou...\n",
              "4        all - time favorite movie ! I have see many mo...\n",
              "                               ...                        \n",
              "24995    be a big fan of the movie , but not for the us...\n",
              "24996    be not go to bother with a plot synopsis since...\n",
              "24997    movie . I do not know . why they would take su...\n",
              "24998    this film on dvd DATE and be gob - smack and f...\n",
              "24999    be a disappointment - none of the nuance of th...\n",
              "Name: review, Length: 25000, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ],
      "source": [
        "train_df['review']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "O477ZV1t1WIO",
        "outputId": "92fa38ef-ac5b-4618-eed1-364f634dcd66"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BarContainer object of 218 artists>"
            ]
          },
          "metadata": {},
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "_, _, hist = plt.hist(train_df.review.apply(lambda text: len(text.split())), bins='auto')\n",
        "hist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXO4xi0u5m8l"
      },
      "source": [
        "Кроме этого, нужно перенумеровать как-то слова."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mIMGE7L-55fs",
        "outputId": "d38ed858-8a26-42a0-c20c-158b62b2d15e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count 12093\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "words_counter = Counter((word for text in train_df.review for word in text.lower().split()))\n",
        "\n",
        "word2idx = {\n",
        "    '': 0,\n",
        "    '<unk>': 1\n",
        "}\n",
        "for word, count in words_counter.most_common():\n",
        "    if count < 10:\n",
        "        break\n",
        "\n",
        "    word2idx[word] = len(word2idx)\n",
        "\n",
        "print('Words count', len(word2idx))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTptlmd1yD3J"
      },
      "source": [
        "**Задание** Сконвертируйте данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "ZPP0cYdJ5VkE"
      },
      "outputs": [],
      "source": [
        "def convert(texts, word2idx, max_text_len):\n",
        "    data = np.zeros((len(texts), max_text_len), dtype=np.int64)\n",
        "\n",
        "    for inx, text in enumerate(texts):\n",
        "        result = []\n",
        "        for word in text.split():\n",
        "            if word in word2idx:\n",
        "                result.append(word2idx[word])\n",
        "        padding = [0]*(max_text_len - len(result))\n",
        "        data[inx] = np.array(padding + result[-max_text_len:], dtype=np.int64)\n",
        "    return data\n",
        "\n",
        "X_train = convert(train_df.review, word2idx, 1000)\n",
        "X_test = convert(test_df.review, word2idx, 1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYb-p5ioyLUZ"
      },
      "source": [
        "Поставим учиться модельку на keras.\n",
        "\n",
        "*Напоминание*: на keras, чтобы обучить модель, нужно\n",
        "1. Определить модель, например:\n",
        "```python\n",
        "model = Sequential()\n",
        "model.add(Dense(1, activation='sigmoid', input_dim=NUM_WORDS))\n",
        "```\n",
        "2. Задать функцию потерь и оптимизатор:\n",
        "```python\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "```\n",
        "\n",
        "3. Запустить обучение:\n",
        "```python\n",
        "model.fit(X_train, y_train,\n",
        "          batch_size=32,\n",
        "          epochs=3,\n",
        "          validation_data=(X_test, y_test))\n",
        "```\n",
        "\n",
        "В NLP чаще всего ставятся задачи классификации, поэтому нужно запомнить такие функции потерь:\n",
        "\n",
        "*   **categorical_crossentropy** - для многоклассовой классификации, в качестве меток должны передаваться one-hot-encoding вектора\n",
        "*   **sparse_categorical_crossentropy** - аналогично предыдущему, но в качестве меток нужно передавать просто индексы соответствующих классов\n",
        "*   **binary_crossentropy** - для бинарной классификации\n",
        "\n",
        "\n",
        "В качестве оптимизатора обычно используют `sgd` или `adam`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncrq09zRxEvN"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "KddjCCo-w50h"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, GlobalMaxPooling1D, Dropout"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dDjri3167vFf",
        "outputId": "4820c8c5-877d-4a3d-cdc2-4890f8ea749a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, 1000, 64)          773952    \n",
            "                                                                 \n",
            " global_max_pooling1d_2 (Gl  (None, 64)                0         \n",
            " obalMaxPooling1D)                                               \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                110       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 774723 (2.96 MB)\n",
            "Trainable params: 774723 (2.96 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=len(word2idx), output_dim=64, input_shape=(X_train.shape[1],)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(units=10, activation='relu'),\n",
        "    Dense(units=10, activation='relu'),\n",
        "\n",
        "    Dense(units=1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lw93gTmq8gZl",
        "outputId": "810756c5-9568-498d-8f5a-2207c1df70f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "196/196 [==============================] - 49s 245ms/step - loss: 0.5757 - accuracy: 0.7430 - val_loss: 0.3724 - val_accuracy: 0.8448\n",
            "Epoch 2/10\n",
            "196/196 [==============================] - 25s 127ms/step - loss: 0.3038 - accuracy: 0.8751 - val_loss: 0.3072 - val_accuracy: 0.8696\n",
            "Epoch 3/10\n",
            "196/196 [==============================] - 16s 80ms/step - loss: 0.2237 - accuracy: 0.9128 - val_loss: 0.2980 - val_accuracy: 0.8734\n",
            "Epoch 4/10\n",
            "196/196 [==============================] - 15s 79ms/step - loss: 0.1632 - accuracy: 0.9406 - val_loss: 0.3135 - val_accuracy: 0.8714\n",
            "Epoch 5/10\n",
            "196/196 [==============================] - 12s 64ms/step - loss: 0.1128 - accuracy: 0.9639 - val_loss: 0.3385 - val_accuracy: 0.8667\n",
            "Epoch 6/10\n",
            "196/196 [==============================] - 8s 44ms/step - loss: 0.0729 - accuracy: 0.9792 - val_loss: 0.3753 - val_accuracy: 0.8655\n",
            "Epoch 7/10\n",
            "196/196 [==============================] - 7s 35ms/step - loss: 0.0435 - accuracy: 0.9908 - val_loss: 0.4195 - val_accuracy: 0.8616\n",
            "Epoch 8/10\n",
            "196/196 [==============================] - 8s 41ms/step - loss: 0.0239 - accuracy: 0.9960 - val_loss: 0.4646 - val_accuracy: 0.8602\n",
            "Epoch 9/10\n",
            "196/196 [==============================] - 6s 31ms/step - loss: 0.0121 - accuracy: 0.9992 - val_loss: 0.5145 - val_accuracy: 0.8585\n",
            "Epoch 10/10\n",
            "196/196 [==============================] - 5s 25ms/step - loss: 0.0065 - accuracy: 0.9998 - val_loss: 0.5482 - val_accuracy: 0.8582\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7b42a7db5990>"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "model.fit(X_train, train_df.is_positive, batch_size=128, epochs=10,\n",
        "          validation_data=(X_test, test_df.is_positive))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBGdVRQTyynD"
      },
      "source": [
        "**Задание** Подсчитайте качество модели на тесте"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, test_tag_df.is_positive, batch_size=1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WQ5M7P-OZPk-",
        "outputId": "1ba0ec1d-d530-437d-80ca-c2274e5ce6f4"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25/25 [==============================] - 0s 9ms/step - loss: 0.5482 - accuracy: 0.8582\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5481529831886292, 0.8582000136375427]"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Выводы\n",
        "Замена сущностей не привела к улучшнию метрики это скорей всего связано с тем что если сущьность отрицательна для некого круго людей то высока вероятность что чем больше людей считаю сущность отрицательной тем в больших текстах сущность будет встречатся как отрицательная"
      ],
      "metadata": {
        "id": "s2aRMXSEtKGF"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}